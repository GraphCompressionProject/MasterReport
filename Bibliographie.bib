%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Macbook at 2018-10-24 16:02:44 +0100 


%% Saved with string encoding Unicode (UTF-8) 


@article{parlebas1972centralite,
  title={Centralit{\'e} et compacit{\'e} d’un graphe},
  author={Parlebas, P},
  journal={Mathematiques et sciences humaines},
  volume={39},
  pages={5--26},
  year={1972},
  publisher={Ecole Pratique des hautes {\'e}tudes, Centre de math{\'e}matique sociale et de statistique}
}


@misc{hennecart2012elements,
	Author = {Hennecart, Fran{\c{c}}ois and Bretto, Alain and Faisant, Alain},
	Publisher = {SPRINGER},
	Title = {El{\'e}ments de th{\'e}orie des graphes},
	Year = {2012}}

@misc{mathieu,
	Author = {Mathieu SABLIK},
	Title = {GRAPHE ET LANGAGE},
	Year = {2018}}

@book{bondy1976graph,
	Author = {Bondy, John Adrian and Murty, Uppaluri Siva Ramachandra and others},
	Publisher = {Citeseer},
	Title = {Graph theory with applications},
	Volume = {290},
	Year = {1976}}

@phdthesis{fages2014exploitation,
	Author = {Fages, Jean-Guillaume},
	School = {Ecole des Mines de Nantes},
	Title = {Exploitation de structures de graphe en programmation par contraintes},
	Year = {2014}}

@techreport{lehman2010mathematics,
	Author = {Lehman, Eric and Leighton, F Thomson and Meyer, Albert R},
	Institution = {Technical report, 2006. Lecture notes},
	Title = {Mathematics for computer science},
	Year = {2010}}

@misc{lopez2003cours,
	Author = {Lopez, Pierre},
	Publisher = {Rapport technique, LAAS-CNRS},
	Title = {Cours de graphes},
	Year = {2003}}

@article{pellegrini2004protein,
	Author = {Pellegrini, Matteo and Haynor, David and Johnson, Jason M},
	Date-Added = {2018-10-24 08:32:01 +0100},
	Date-Modified = {2018-10-24 08:32:01 +0100},
	Journal = {Expert review of proteomics},
	Number = {2},
	Pages = {239--249},
	Publisher = {Taylor \& Francis},
	Title = {Protein interaction networks},
	Volume = {1},
	Year = {2004}}

@phdthesis{lemmouchi2012etude,
	Author = {Lemmouchi, Slimane},
	Date-Added = {2018-10-23 23:12:05 +0100},
	Date-Modified = {2018-10-23 23:12:05 +0100},
	School = {Universit{\'e} Claude Bernard-Lyon I},
	Title = {Etude de la robustesse des graphes sociaux {\'e}mergents},
	Year = {2012}}

@inproceedings{guillaume2002web,
	Author = {Guillaume, Jean-Loup and Latapy, Matthieu},
	Booktitle = {Actes d'ALGOTEL'02 (Quatri{\`e}mes Rencontres Francophones sur les aspects Algorithmiques des T{\'e}l{\'e}communications)},
	Date-Added = {2018-10-23 21:11:29 +0100},
	Date-Modified = {2018-10-23 21:11:29 +0100},
	Title = {The Web graph: an overview},
	Year = {2002}}

@incollection{levene2004navigating,
	Author = {Levene, Mark and Wheeldon, Richard},
	Booktitle = {Web Dynamics},
	Date-Added = {2018-10-23 21:09:31 +0100},
	Date-Modified = {2018-10-23 21:09:31 +0100},
	Pages = {117--151},
	Publisher = {Springer},
	Title = {Navigating the world wide web},
	Year = {2004}}

@book{bac,
	Author = {Michel Rigo},
	Date-Added = {2018-10-22 13:20:43 +0100},
	Date-Modified = {2018-10-22 13:22:53 +0100},
	Publisher = {Universit{\'e} de li{\`e}ge, Facult{\'e} des sciences D{\'e}partement de math{\'e}matiques},
	Title = {Th{\'e}orie des graphesorie des graphes},
	Year = {2010}}

@manual{DUT,
	Author = {Ph. Roux},
	Date-Added = {2018-10-22 12:25:53 +0100},
	Date-Modified = {2018-10-22 12:28:24 +0100},
	Month = {f{\'e}vrier},
	Title = {Th{\'e}orie des graphes},
	Year = {2014}}

@techreport{Pres,
	Author = {Jean-Charles R{\'e}gin, Arnaud Malapert},
	Date-Added = {2018-10-22 10:57:49 +0100},
	Date-Modified = {2018-10-22 10:58:29 +0100},
	Title = {Th{\'e}orie des Graphes},
	Year = {2016}}

@book{IUTLyonInformatique,
	Date-Added = {2018-10-22 09:20:34 +0100},
	Date-Modified = {2018-10-22 10:55:04 +0100},
	Publisher = {IUT Lyon Informatique},
	Title = {Quelques rappels sur la th{\'e}orie des graphes},
	Year = {2012}}

@book{muller,
	Author = {Didier M{\"u}ller},
	Date-Added = {2018-10-22 09:13:39 +0100},
	Date-Modified = {2018-10-22 09:16:03 +0100},
	Publisher = {Commission romande de math{\'e}matique (CRM).},
	Title = {Introduction {\`a} la th{\'e}orie des graphes},
	Year = {2012}}

@article{karypis2000multilevel,
	Author = {Karypis, George and Kumar, Vipin},
	Date-Added = {2018-10-17 20:22:24 +0100},
	Date-Modified = {2018-10-17 20:22:54 +0100},
	Journal = {VLSI design},
	Number = {3},
	Pages = {285--300},
	Publisher = {Hindawi},
	Title = {Multilevel k-way hypergraph partitioning},
	Url = {https://www.cs.york.ac.uk/rts/docs/DAC-1964-2006/PAPERS/1999/DAC99_343.PDF},
	Volume = {11},
	Year = {2000},
	Bdsk-Url-1 = {https://www.cs.york.ac.uk/rts/docs/DAC-1964-2006/PAPERS/1999/DAC99_343.PDF}}

@article{hespanha2004efficient,
	Abstract = {



									Spectral Clustering

	Le cas. des graphes non orientee 

	Dans ce travaille, une analyse du spectre de la matrice d'adjacence est effectuee en se basant sur ses valeurs/vecteurs propores.

	Un vecteur de partition est un vecteur de taile |V| dont chaque entree represente le num de partition auquel le noeuds est affectee.
	Une matrice de k-partition H est une matrcie dont les valeurs sont binaire et chaque colonne ci represente par 1 les noeuds de la partition Vi et 0 les autres noeuds. De ce fait,on aura que H'.H =diag{|Vi|}.
	Le cout d;une partition P est C(P) = 1′nA1n − trace(Π′AΠ), ou A est la matrice des couts du graphe et 1n est une vecteur dont les valeurs sont des 1.

	Le probleme  d'optimisation peut alors se formuler ainsi :
		maximize trace(Π′ AΠ) subject to πvj ∈ {0, 1}, ∀v, j
		Π orthogonal 
		1nΠ1k = n 
		1nΠei ≤ l, ei est le ieme vecetur de la base canonique

	Dans ce travaille on reduit la resolution aux cas ou la somme des couts des ses aretes est = 1 (Normalisation : lorsque le degree est elevee alors le couts doit etre petit dans la moyenne des cas) dasn ce cas la matrice est dite doublement stochastique.

	On suppose que Π := UZ.Il propose pou resoudre ce probleme 2 algo:
		- Clustering algorithme : pour calculer le Z avec une heuristique permettant de respecter le meiux les conditions 2 et 3 
		- Algorithme de projection : Pour calculer le Π optimale 

Rappel : Sur le spectre d'une matrice

	- valeur propre Av=lamda.v ==> lamda valeur propre
	- A = Q-1 . B . Q ou B est une matrice diagonale et les bii = valeurs propre de A et Q est une matrice dont les colonnes sont des vecteurs propres


Note :

	- Le code est fourni en annexe _MatLab Code_},
	Author = {Hespanha, Joao P},
	Date-Added = {2018-10-17 18:53:23 +0100},
	Date-Modified = {2018-10-20 22:48:21 +0100},
	Journal = {Santa Barbara, CA, USA: University of California},
	Publisher = {Citeseer},
	Title = {An efficient matlab algorithm for graph partitioning},
	Year = {2004}}

@article{blondel2008fast,
	Abstract = {


	Le principe de cette methode est le suivant :
		-l'algo est composee de deux phase 
			- Phase01 : le temp d'execution de cette phase depend enormement de l'ordre des noeuds

				- chaque noeuds est considere comme une communautee 
				- Pour chauque noeud i on considere ses voisins j 
					 on evalue le gain de modularite apportee en supprimant le noeud i et on le rajoutant a la communautee de j
				- On rajoute i a la communautee apportant max gain positive si aucun gain n est positive alors x reste dans sa communautee 

			- Phase 02 : consiste en la construction d'un nouveau graphe dont 
					- les neouds sont les communautee de la phase 01
					- le poids des aretes = 
							- la somme des poids des aretes liant les noeuds des deux communautee 
							- les aretes interne aux communautee deviennet des boucles dont le poids est la somme tjrs
			
			- reappliquer la phase 01 jusqu a ce soit :
					- plus de changement
					- maximum de modularite est atteint
					},
	Author = {Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
	Date-Added = {2018-10-17 18:05:37 +0100},
	Date-Modified = {2018-10-17 18:20:23 +0100},
	Journal = {Journal of statistical mechanics: theory and experiment},
	Number = {10},
	Pages = {P10008},
	Publisher = {IOP Publishing},
	Title = {Fast unfolding of communities in large networks},
	Volume = {2008},
	Year = {2008}}

@inproceedings{giatsidis2011evaluating,
	Abstract = {


	Les sous-graphe de communautee sont caracterisee par leur leins dense. La mesure de la qualitee d'une communautee est tres importante. En effet plusieurs mesure d'evaluation existe dans lalitterature, ce travaille evalue les communautee en se basant sur le concepte de k-core.




Definitions:
	- we define the k-core of a graph as the maximum size subgraph H of G where ∆(H) ≥ k , where k is a positive integer.
		en terme plus simple c'est le graphe initiale en iliminant les noeuds ayant un degree < k .
	- The degeneracy of G is defined as follows.  δ∗(G) = max{∆(H) | H ⊆ G}.



Beatiful Expressions : _To use in the abstract_

	-  In all cases, due to the economic aspects of these networks, the ranking of individual nodes is a cornerstone concept.},
	Author = {Giatsidis, Christos and Thilikos, Dimitrios M and Vazirgiannis, Michalis},
	Booktitle = {Advances in Social Networks Analysis and Mining (ASONAM), 2011 International Conference on},
	Date-Added = {2018-10-17 15:49:25 +0100},
	Date-Modified = {2018-10-24 14:49:19 +0100},
	Organization = {IEEE},
	Pages = {87--93},
	Title = {Evaluating cooperation in communities with the k-core structure},
	Url = {https://www.researchgate.net/profile/Michalis_Vazirgiannis/publication/221273470_Evaluating_Cooperation_in_Communities_with_the_k-Core_Structure/links/56b89ce608ae44bb330d355c/Evaluating-Cooperation-in-Communities-with-the-k-Core-Structure.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Michalis_Vazirgiannis/publication/221273470_Evaluating_Cooperation_in_Communities_with_the_k-Core_Structure/links/56b89ce608ae44bb330d355c/Evaluating-Cooperation-in-Communities-with-the-k-Core-Structure.pdf}}

@inproceedings{kang2011beyond,
	Abstract = {

	Le travaille present repond a deux questions importantes : comment compressee un graphe efficacement ?comment detecter les communautee dans un grpahe donnee? qui sont deux questions en etroite relation. En effet, si on trouve les bonnes communautee alors on aura une meilleur compression vues que les neouds d'une d'une mm communautee contiennent des informations redondantes.

	Au debut l'approche utilisee est celle du « cave Man » ou on considere qu'un individue connait bocoup d'informations sur  les individue (noeuds) qui sont avec lui dans une meme caverne (clique) et peu d\informations sur les individue dans les autres cavernes. Cependant, les graphes du monde reel n'obeissent pas generalement a cette regle. Ils suivent une distributions de degr{\'e}s de loi de puissance avec peu de n{\oe}uds "hub" ayant des degr{\'e}s tr{\`e}s {\'e}lev{\'e}s et la majorit{\'e} des n{\oe}uds ayant des degr{\'e}s faibles. Cette constation a remis en cause le principe du  « cave man » car les hub sont connectee a la majoritee des noeuds sonnant ainsi une large caverne.

	La methode proposee, baptisee SLASHBURN, est basee sur l'exploitation les hub et leurs visinage en paratant du fait que les graphe reel peuvent etre facilement partitionner par ces hubs. Les communautees defini ainsi sont caracterisee dans la matrice d'adjacence par des clocks maigre (!!!!). Ce travaille montre que l'ordonnoncementde ces noeuds d'une maniere prudente permet d'avoir une representation compacte de matrcie d'adjacence.

	Le princpe de cette methode consiste en 3 etapes :
		1. Algorithm 1: SLASHBURN
I			Input: Edge set E of a graph G = (V,E), a constant k(default = 1).
			Output: Array Γ containing the ordering V → [n].
				1: Remove k-hubset from G to make the new graph G′.
					Add the removed k-hubset to the front of Γ.
				2: Find connected components in G′. Add nodes in
					non-giant connected components to the back of Γ, in the decreasing order of sizes of connected components they belong to.
				3: Set G to be the giant connected component(GCC) of G′. Go to step 1 and continue, until the number of nodes in the GCC is smaller than k.

	Complexite :
		- temporelle  : O(|E| + |V |log|V |)i ou i est le nombre d'iterations 
		- spaciale :	O(V ) 



Information Supplementaire :

	- HADOOP, an open source MAPREDUCE framework can be used to  show the performance implication of an algo for large scale graph mining on distributed platform,

	- LiveJournal is one of the dataset that is very hard to compress

	- real world, power-law graphs and ran- dom(Erdo ̋s-Re ́nyi [15]) graphs? 

},
	Author = {Kang, U and Faloutsos, Christos},
	Booktitle = {Data Mining (ICDM), 2011 IEEE 11th International Conference on},
	Date-Added = {2018-10-17 12:18:32 +0100},
	Date-Modified = {2018-10-17 15:36:18 +0100},
	Organization = {IEEE},
	Pages = {300--309},
	Title = {Beyond'caveman communities': Hubs and spokes for graph compression and mining},
	Year = {2011}}

@article{hernandez2014compressed,
	Author = {Hern{\'a}ndez, Cecilia and Navarro, Gonzalo},
	Date-Added = {2018-10-17 10:15:10 +0100},
	Date-Modified = {2018-10-17 10:15:23 +0100},
	Journal = {Knowledge and information systems},
	Number = {2},
	Pages = {279--313},
	Publisher = {Springer},
	Title = {Compressed representations for web and social graphs},
	Url = {http://repositorio.uchile.cl/bitstream/handle/2250/126521/Compressed%20representations%20for%20web%20and%20social%20graphs.pdf?sequence=1},
	Volume = {40},
	Year = {2014},
	Bdsk-Url-1 = {http://repositorio.uchile.cl/bitstream/handle/2250/126521/Compressed%20representations%20for%20web%20and%20social%20graphs.pdf?sequence=1}}

@inproceedings{buehrer2008scalable,
	Author = {Buehrer, Gregory and Chellapilla, Kumar},
	Booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
	Date-Added = {2018-10-17 10:12:39 +0100},
	Date-Modified = {2018-10-17 10:13:38 +0100},
	Organization = {ACM},
	Pages = {95--106},
	Title = {A scalable pattern mining approach to web graph compression with communities},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.9435&rep=rep1&type=pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.9435&rep=rep1&type=pdf}}

@article{liu2015empirical,
	Abstract = {

												                     VoG Overlapp

Comparaison entre 5differentes technique de detection de communitee et de clustering en terme de la taille et de la qualite du resultat de ses dernieres. Les cinq methode sont : METIS, Louvain, spectral clustering, SlashBurn et la methode proposee dans cet article k-core-based clustering notee KCBC.

},
	Author = {Liu, Yike and Shah, Neil and Koutra, Danai},
	Date-Added = {2018-10-17 10:10:12 +0100},
	Date-Modified = {2018-10-17 12:10:43 +0100},
	Journal = {arXiv preprint arXiv:1511.06820},
	Title = {An empirical comparison of the summarization power of graph clustering methods},
	Url = {https://arxiv.org/pdf/1511.06820.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1511.06820.pdf}}

@article{liureducing,
	Author = {Liu, Yike and Safavi, Tara and Shah, Neil and Koutra, Danai},
	Date-Added = {2018-10-17 10:09:26 +0100},
	Date-Modified = {2018-10-17 10:09:36 +0100},
	Title = {Reducing Million-Node Graphs to a Few Structural Patterns: A Unified Approach},
	Url = {http://www.mlgworkshop.org/2016/paper/MLG2016_paper_29.pdf},
	Bdsk-Url-1 = {http://www.mlgworkshop.org/2016/paper/MLG2016_paper_29.pdf}}

@inproceedings{shah2015timecrunch,
	Author = {Shah, Neil and Koutra, Danai and Zou, Tianmin and Gallagher, Brian and Faloutsos, Christos},
	Booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2018-10-17 10:08:37 +0100},
	Date-Modified = {2018-10-17 10:08:52 +0100},
	Organization = {ACM},
	Pages = {1055--1064},
	Title = {Timecrunch: Interpretable dynamic graph summarization},
	Url = {https://web.eecs.umich.edu/~dkoutra/papers/Timecrunch_KDD15.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://web.eecs.umich.edu/~dkoutra/papers/Timecrunch_KDD15.pdf}}

@inproceedings{dunne2013motif,
	Author = {Dunne, Cody and Shneiderman, Ben},
	Booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	Date-Added = {2018-10-17 10:07:52 +0100},
	Date-Modified = {2018-10-17 10:07:52 +0100},
	Organization = {ACM},
	Pages = {3247--3256},
	Title = {Motif simplification: improving network visualization readability with fan, connector, and clique glyphs},
	Year = {2013}}

@article{brisaboa2014compact,
	Abstract = {	
	Ce travaille est une continuitee de « k2-trees for compacte web graph representattion ».


Definitions:

	- Graphe du Web : est un graphe orientee ou chaque noeud represente une page web et les aretes represente les hyperliens entre les pages. 

Information Supplementaire :
	
	- HIts : l'un des plus important algorithme pour trouver les hubs et les autoritee sur le web. Commence par selectionner un ensemble aleatoire de page et trouver les sous graphes induit par ces pages qui ne sont rien d'autre que les pages accessibles via ces noeuds ou les predecceurs de ces noeuds.},
	Author = {Brisaboa, Nieves R and Ladra, Susana and Navarro, Gonzalo},
	Date-Added = {2018-10-03 18:52:20 +0100},
	Date-Modified = {2018-10-03 19:23:51 +0100},
	Journal = {Information Systems},
	Pages = {152--174},
	Publisher = {Elsevier},
	Read = {1},
	Title = {Compact representation of web graphs with extended functionality},
	Url = {http://repositorio.uchile.cl/bitstream/handle/2250/126520/Compact%20representation%20of%20Webgraphs%20with%20extended%20functionality.pdf?sequence=1},
	Volume = {39},
	Year = {2014},
	Bdsk-Url-1 = {http://repositorio.uchile.cl/bitstream/handle/2250/126520/Compact%20representation%20of%20Webgraphs%20with%20extended%20functionality.pdf?sequence=1}}

@article{seo2018effective,
	Author = {Seo, Hojin and Park, Kisung and Han, Yongkoo and Kim, Hyunwook and Umair, Muhammad and Khan, Kifayat Ullah and Lee, Young-Koo},
	Date-Added = {2018-10-03 11:00:47 +0100},
	Date-Modified = {2018-10-03 11:00:47 +0100},
	Journal = {The Journal of Supercomputing},
	Pages = {1--15},
	Publisher = {Springer},
	Title = {An effective graph summarization and compression technique for a large-scaled graph},
	Year = {2018}}

@article{khan2017faster,
	Abstract = {

	Compression de graphe ponderee est utile pour modeliser des situation de la vie reel. En effet, une compression qui ne prend pas en compte les poids des arets ne peut pa modeliser les relation fortes et faibles entre les noeuds .
	
	Cet article propose la solution a deux probleme majeurs qui sont : BWGS et PWGS. 
	Pour minimiser les calcul, on utilise  LSH qui aide a restreindre le calcul de similarite aux noeuds les plus propable d'etre similaire. La mesure de similarite est calculee avec le coefficient de Jaccard.
	
	L'approche BWGS ne peut etre utilisee en tant que tel pour PWGS car elle ignore les poids, qui ont une importance dans la mesure d'influence, des aretes durant la phase de hachage. C'est pourquoi les auteurs proposent un autre schema de hashage pour le PWGS. De ce fait, ils ont obtenue une solution qui n'en seulement permet de realiser la compression dans un temps optimale mais aussi de prendre en compte les poids des differents noeuds.

	Pour trouver l'ensemble formant un super- noueds dans le graphe et dans le cas des deux approches, on choisit un noeuds q et on choisiit les noeuds qui ont un EDR > seuil (si proche de 0.5 => mauvise compression et si trop petit => des erreurs). 


	Le BWGS : dont le but est de calculer Sg dans un temps raisonnable et de maniere consice pour qu'il suppote l'execution des differents algorithmes d'analyse en memoire centrale. Pour repondre au premier besoin on rassemble a chauq iteration un ensemble de noeuds et non pas une pair uniquement tant dis que que pour repondre au deuxieme besoin on utilise le taux de compression cr(Sg) = |Es| / |E| . Le probleme donc se formule ainsi :

			Prob 01 : Etant G un graph non orientee et un seuil de compression 0<cr<1 , trouver Sg dans un temp lineaire ayant un cr(Sg)<cr  ,une ditance minimale avec G et qui introduit un min d'erreurs d'aretes. 

		- Creating a summary graph for the BWGS problem
			- Validating the use of the Jaccard coefficient 
					
			- Locating compressible nodes by using LSH
			- Merging compressible nodes
				apres avoir regroupee les noeuds en se basant sur LSH, on choisie aleatoirement a chauqe iteration un noeuds q et les noeuds qui apparaisssent avec lui dans la table de hashage = ScandNq qu'on ordonne selon EDR et iterativement on elimine les noeuds qui viole les contraintes de seuil et la difference de piods =SCompNq. On observe que si on fusionnne les noeuds ScompNq directement on obtient un taux d'erreurs elevees c'est pourquoi on fusionne les neouds les plus similaire dans un premier lieu puis on rajoute les autres sequentiellement (certain noeuds sont eliminer psk le calcul EDR est influencer par la fusion des noeuds precedent)



	PWGS: BWGS ne permet d'analyser l'influence dans G car il est construit en se basant sur la similiratee de voisinage et non l'influence qui est represetentee par les poids des artes. En effet , chaque poids wi de u vers v montre le degree d'influence de v sur u. 

			Prob 02 : Etant Pg un graph personnalisee et un noeuds influenceur vinf , trouver son compressee Sg ayant k super noeuds et l super-arets  ,une ditance minimale avec G et qui introduit un min d'erreurs d'aretes. 

		- Creating a summary graph for the PWGS problem
			La taille de Sg est depend fortement de la taille de Pg c'est pourkoi on ne prends en compte que k supernoueds et l super-arets qui sont, pour les noeuds trouver avec DFS BFS q partie de vinf ,et pour les poids sont calculer en tant que debit avec equation 4 de l'article.

			














Questions : 
	- dans la definition du probl 02 ,j'ai pa compri le k noeuds et l aretes ca concerne qui esk Pg ou Sg ??
	- contradiction de la definition 02 avec explication qui viens avant !!!!









Definitions:

	- Le compressee : The summary SG(Vs,Es,ws) of a weighted graph G(V,E,w) is a compressed representation whose nodes are called super nodes Vs and edges are referred to as super edges Es. w of every (u,v)⊆E represents the edge weight, whereas the semantics of ws (us , vs ) ⊆ Es differ for our two types of summarization problems. 

	- Chaque super noeud vs ∈ Vs contient soit une paire ou un ensemble de noeuds {v1,...,vl}∈V ou il est preservee comme il est , i.e., vs ∈Vs ⇒ vs ∈V. De facon similaire, les aretes sont agregger pour former des super arete dans SG.
	- Chaque super noeuds represente une clique,bi-clique, ou un. sous graphe bipartie
	- La creation des super noeuds peut introduire des erreurs de deux types : 
			|------> insertion de nouvelle aretes : 
			|------> suppresssion d'aretes existante si il viole les conditions: EDR > seuil

	- (Personalized graph). Given a directed graph G and an influential node vinf ∈V, a personalized graph PG(Vp,Ep,wp) is a directed subgraph of G where every vp ∈Vp ⇒ vp ∈V and is accessible from vinf.
	- Un super neuds est ensemble de neouds ayant un voisinage simialire et une difference de poids minimale pour chaque arete.

	- (Set of Candidate Similar Nodes). Given a set of nodes S = {v1, .., vi, .., vm} ∈ V, q ∈ V, an EDR threshold tN, and an edge weight difference threshold tEw, we call S the Set of Candidate Similar Nodes SCandNq if the EDR of any vi ∈ S with q is below tN or their difference in edge weights is above tEw.

	- (Set of Compressible Nodes). Given an SCandNq, q∈V, an EDR threshold tN, and edge weight difference thresh- old tEw, its largest subset is the Set of Compressible Nodes SCompNq if the EDR of every vi ∈ SCandNq with q is above tN and their difference in edge weights is below tEw.

},
	Author = {Khan, Kifayat Ullah and Dolgorsuren, Batjargal and Anh, Tu Nguyen and Nawaz, Waqas and Lee, Young-Koo},
	Date-Added = {2018-10-03 10:57:36 +0100},
	Date-Modified = {2018-10-05 12:41:04 +0100},
	Journal = {Information Sciences},
	Pages = {237--253},
	Publisher = {Elsevier},
	Title = {Faster compression methods for a weighted graph using locality sensitive hashing},
	Url = {https://www.researchgate.net/profile/Kifayat_Khan4/publication/318768173_Faster_Compression_Methods_for_a_Weighted_Graph_using_Locality_Sensitive_Hashing/links/59c067c80f7e9b48a29bb086/Faster-Compression-Methods-for-a-Weighted-Graph-using-Locality-Sensitive-Hashing.pdf},
	Volume = {421},
	Year = {2017},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Kifayat_Khan4/publication/318768173_Faster_Compression_Methods_for_a_Weighted_Graph_using_Locality_Sensitive_Hashing/links/59c067c80f7e9b48a29bb086/Faster-Compression-Methods-for-a-Weighted-Graph-using-Locality-Sensitive-Hashing.pdf}}

@inproceedings{brisaboa2009k,
	Abstract = {

	Plusieurs algorithmes existent dans la litterature pour explorer les structures de graphe mais la problematique posee de nos jours est celle de la grande masse de donnee (BIG DATA). Ce qui a poussee plusieurs chercheurs a considere la compression de graphe qui a pour but d'offrir une representation compacte permettant une navigation rapide sans decompression. De ce fait, les algorithmes classiques pourront etre executee dans la memoire centrale meme si la taille du graphe est tres importante.

	Ce travail exploitent les propriete de la matrice d'adjacence avec une technique generaliste tirant profis du clustering. La representation proposee permet de faire un parcours directe ou inverse du graphe. Ces derniere sont en nombre de 3:
	- localitee de reference : plusieurs 1 sont placee autour de la diagonale
	- copy property : les lignes similaires sont commun dans la matrice
	- asym{\'e}trie de distribution : quelques colonnes ont pluseiurs 1 mais la plupart ont quelques 1 (matrice creuse).
	
	Dans cette approche la matrice d'adjacence est reporesentee par un k2-tree qui est un arbre dont chaque noeuds contient un seul bit de donnee: 1 pour les noeuds interne et 0 pour les feuilles a l'exception des de dernier niveau ou les valeurs des noeuds representent des valeurs de la matrice. Chaque noeuds possedent  possede k2 fils si sa valeur est 1 et 0 fils sinon.Deux cas se presente, si n=k2 alors alors on divise la matrice en des sous matrice de taille k2 sinon on complete la matrice avec des 0 (a droite et en bas ) pour arriver a la prochaine puissance carree. Le resultat est dons k2 sous matrices dont chacune represente un noueds fils dans l'arbre de valeurs 0 si toutes ses cellules sont nulles et de valeurs 1 sinon.

	L'arbre est representee en memoire sous forme de deux tableax :
		- T : contenant les bits des noeuds ordonnee par niveaux  a l'exception du dernier niveau
		- L : contenant les bits du derniers niveaux.
	Les algorithmes de parcours directe et inverse sont enoncee dans l'article et sont facile comprendre. Le choix du k affecte les performances de cet aproche. En effet, plus le k est petit plus on a besoin d'espace memoire et moins de matrices on aura. De ce fait, les auteurs propose une hybridation par rapport au valeurs de k ou des grandes valeurs de k seront utilisee pour les premiers niveaux et de petite valeur de k pour les derniers niveaux de l'arbre.
	
Definitions:

	- Matrice d'adjacence du Graphe du Web: est une matrice carree de {aij} de taille nxn, ou chaque ligne ou colonne represente une page web. Une cellule aij=1 si il y a un hyperlien dans la page i vers la page j et 0 sinon.

	- la complexitee au pire des cas est : logk2(n2) noeuds => k2 m logk(n).


NB: 
	- On part de l'hypothese que les pages web sont ordonnee par ordre alphabetique ce qui met les pages du meme domaine dans des positions proches dans la matrice

Citation Importante:
 
	- Web Graphe Framework de Boldi and vigna.
	- Claude and navaro Re_pair for webGraph. with reverse navigation },
	Author = {Brisaboa, Nieves R and Ladra, Susana and Navarro, Gonzalo},
	Booktitle = {International Symposium on String Processing and Information Retrieval},
	Date-Added = {2018-10-03 10:56:33 +0100},
	Date-Modified = {2018-10-04 00:26:02 +0100},
	Organization = {Springer},
	Pages = {18--30},
	Title = {k 2-trees for compact web graph representation},
	Url = {https://pdfs.semanticscholar.org/7d98/d729058067dcd520ce670c83842cafa289a0.pdf},
	Year = {2009},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/7d98/d729058067dcd520ce670c83842cafa289a0.pdf}}

@article{koutra2015summarizing,
	Abstract = {
	

	L'idee de ce travaillle est de construire un vocabulaire des types de sous-graphes (etoile, clique, chaines,bi-clique) qui apparaissent frequemment dans les graphe reel et de trouver la representation la plus succincte d'un graphe en terme de ce vocabulaire qui est : cliques, near-cliques, stars, chaines, near-bipartite, cores. L'evaluation de cette methode ce fait a travers le principe MDL ce qui permet d'avoir une efficacitee independante des parametre d'entree. 


	La problematique 01 est donc de trouver a partir d'un graphe en entree, la liste des sous-graphe, eventuellement qui se chevauchent (noeuds en commun mais non arete en commun), qui decrit le graphe de la maniere la plus succincte.

	La problematique 02 est de trouver le model M le plus petit en terme de longueur de bits pour representer G dont la longueur est de L(G,M) = L(M) + L(E) , a partir du graphe G, sa matrice d'adjacence A et un vocabulaire de structure.
	
	L'ensemble des modele M' est la permutation de tout les sous-ensembles possibles de de C = union  {tout les structure possible d'un type x}

	    1. Pour l'encodage du modele M on utilise la formule suivante:

	    		L(M) = LN( |M|+1 ) + log			+ 
	    				|			|			|--->pour chaque structure s on encode son type x(s) avec un code de prefixe optimale et sa structure
	    				|			|------------------------> encodage du nombre de structure de chaque type x dans le modele M
	    				|---------------------------------------------> Nombre de structure dans le modele M
	    	    L'encodage du modele depends de la longueur des structure s notee L(s) qui se calcule selon le type de la structure x(s):
	    		- s = Clique : L(s) = LN( |s| ) +log(n |s|) // encodage du nombre de noeuds + leurs ids
	    		- s =  near-clique : L(s) = LN( |s| ) +log(n |s|) + {\ldots}{\ldots} // encodage du nombre de noeuds + leurs ids + nombre d'arete presente et le nombre d'arete manquante. c'est un encodage exacte 
	    		- s =  Core Bipartie : A et B sont deux cores non vides et ne possedant aucun noeuds en commun. ayant des aretes externe de l'un vers l'autre mais aucune arete interne. // la taille de A et la taille de B ainsi que les ids des noeuds.
	    		- s =  near Core Bipartie :  // la taille de A et la taille de B ainsi que les ids des noeuds +  nombre d'arete presente et le nombre d'arete manquante. c'est un encodage exacte 
	    		- s =  Etoile : cas particulier des core bi-partie ou A= noeuds centrale et B= reste // Le nombre de noeuds externe + identification du hub + identification des spokes ????
	    		-s =  chaine : // la longueur de la chaine + identification des noeuds dans l'ordre

	    2.Pour l'erreure E : E=E+. et E- ou E+ = la partie de A modeliser par M et qui contient des aretes superflux et E- =la partie de A non modeliser par M et dont M manque d'aretes 
			L(E) = L(E-) + L(E+) 

	L'algorithme est constituee principalement de 3 etapes essetiels :
		- Generation des sous-graphes : technique de clustering ou de detection de communautee peuvent etre utilisee. Dans le cas de l'article, la methode utilisee est SLASHBURN.

		- Etiquetage des sous-graphes :
				- Etiquetage des structure parfaite : 
					- Le teste pour les cliques ou les chaines est basee sur les le degree de distribution
					- Bipartie si la sa valeur propre maximale est minimale sont de meme ordre de grandeur ( =? )
							|---> pour trouver les deux xensembles de noeuds on utilise BFS avec coloriage des noeuds . si l'un des ensemble ne contient qu'un seul element alors c'est une etoile et non un bipartie
				-  Etiquetage des structure approximative : 
					- le sous graphe est encodee dans tout les cas possible et la structure ayant un cout minimale en terme de MDL et conservee 

		- l'assemblage : Le probleme de trouver M parmis M' est NP-difficile c'est pourquoi on a recours a des heuristiques .
			pour reduire l'espace de recherche, on associe a chauqe structure condidate une mesure de qualitee == nbre de bits gagnee en codant le sous graphe comme une structure x
			Les heuristics constituee sont :
				- Plain : approche de base donne toutes les structures condidates comme summary
				- TOP -k : Selectionne les k meilleurs condidats qui sont ordonnee dans un ordre decroisssant de la mesure de qulitee
				- GREEDY'NForget : considere chaque structure condidate sequentiellement et l'inclue dasn le modele si elle n'augemente pas son cout. Il est a notee que cette heuristique est couteuse en temps de calculs et donne de meilleurs resulats avec les petite instance de sous graphe.
			
	VoG utilsent les trois heuristiques et choisie la meilleurs compression. Il a une complexite temporelle presque lineaire par rapport au nombre de noeuds.
	
	Le but de ce travaille n'est pas de faire un compression mais d'extraire les motifs les plus signifiant d'un graphe en utilisant la compression comme le moyen le plus efficace.

Acronyme:
	- VoG: Vocabulary-based-summarization of Graphs.
	- MDL : Minimum Description Length

NB:
	- Cette compression est applicable aux graphes non-orientee et sans boucles mais peut etre generalisee.
	- Dans la matrice d'adjacence :
		- Une clique : une sous matrice carree plaine de 1
		- Near-clique : une sous matrice carree dont la plupart des valeurs sont des 1
		- etoile : un L inversee
		- chaine : ligne parallele a la diagonale

	- Erreur E = M xor A d'ou on a un algorithmes de compression sans perte.

	-LN est le codage MDL le plus optimale pour representer les entiers >=1 
	
},
	Author = {Koutra, Danai and Kang, U and Vreeken, Jilles and Faloutsos, Christos},
	Date-Added = {2018-10-03 10:55:19 +0100},
	Date-Modified = {2018-10-08 20:24:59 +0100},
	Journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	Number = {3},
	Pages = {183--202},
	Publisher = {Wiley Online Library},
	Title = {Summarizing and understanding large graphs},
	Url = {https://arxiv.org/pdf/1406.3411.pdf},
	Volume = {8},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1406.3411.pdf}}
