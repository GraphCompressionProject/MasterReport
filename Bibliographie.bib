%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Macbook at 2018-10-31 17:29:51 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{liu2018graph,
	Abstract = {

	This survey is a structured, comprehensive overview of the state-of-the-art methods for summarizing graph data. 
			- the motivation behind, and the challenges of, graph summarization. 
			- categorize summarization approaches by the type of graphs taken as input and further organize each category by core methodology. 
			- applications of summarization on real-world graphs and conclude by describing some open problems in the field.



La reduction de donnee est devenue primordiale de nos jours vue la quantitee enorme de donnee disponible (produite par nos activite) et la necessitee de les analyser. Comme elle facilite l'extraction et la compression de l'information, la communautee scientifique s'est investie dans ce domaine ce qui a donnee naissance a plusieurs travaux dans diffrents domaines.

	Dans ce survey, les auteurs s'interessent aux donnee liees, autreemnt dit les Â« Graphes Â» qui sont omnipresents dans presque tout les domaines.



Remarque:

	-  A summary is application- dependent and can be defined with respect to various goals: it can preserve specific structural patterns, focus on some network entities, preserve the answers to graph queries, or maintain the distributions of graph properties},
	Author = {Liu, Yike and Safavi, Tara and Dighe, Abhilash and Koutra, Danai},
	Date-Added = {2018-10-31 16:23:18 +0100},
	Date-Modified = {2018-10-31 16:26:35 +0100},
	Journal = {ACM Computing Surveys (CSUR)},
	Number = {3},
	Pages = {62},
	Publisher = {ACM},
	Title = {Graph Summarization Methods and Applications: A Survey},
	Volume = {51},
	Year = {2018}}

@article{sethi2014data,
	Author = {Sethi, Gaurav and Shaw, Sweta and Vinutha, K and Chakravorty, Chandrani},
	Date-Added = {2018-10-30 09:40:46 +0100},
	Date-Modified = {2018-10-30 09:40:46 +0100},
	Journal = {International Journal of Computer Science and Information Technologies},
	Number = {4},
	Pages = {5584--6},
	Title = {Data compression techniques},
	Volume = {5},
	Year = {2014}}

@article{lelewer1987data,
	Author = {Lelewer, Debra A and Hirschberg, Daniel S},
	Date-Added = {2018-10-29 23:25:32 +0100},
	Date-Modified = {2018-10-29 23:25:32 +0100},
	Journal = {ACM Computing Surveys (CSUR)},
	Number = {3},
	Pages = {261--296},
	Publisher = {ACM},
	Title = {Data compression},
	Volume = {19},
	Year = {1987}}

@misc{hennecart2012elements,
	Author = {Hennecart, Fran{\c{c}}ois and Bretto, Alain and Faisant, Alain},
	Publisher = {SPRINGER},
	Title = {El{\'e}ments de th{\'e}orie des graphes},
	Year = {2012}}

@misc{mathieu,
	Author = {Mathieu SABLIK},
	Title = {GRAPHE ET LANGAGE},
	Year = {2018}}

@book{bondy1976graph,
	Author = {Bondy, John Adrian and Murty, Uppaluri Siva Ramachandra and others},
	Publisher = {Citeseer},
	Title = {Graph theory with applications},
	Volume = {290},
	Year = {1976}}

@phdthesis{fages2014exploitation,
	Author = {Fages, Jean-Guillaume},
	School = {Ecole des Mines de Nantes},
	Title = {Exploitation de structures de graphe en programmation par contraintes},
	Year = {2014}}

@techreport{lehman2010mathematics,
	Author = {Lehman, Eric and Leighton, F Thomson and Meyer, Albert R},
	Institution = {Technical report, 2006. Lecture notes},
	Title = {Mathematics for computer science},
	Year = {2010}}

@misc{lopez2003cours,
	Author = {Lopez, Pierre},
	Publisher = {Rapport technique, LAAS-CNRS},
	Title = {Cours de graphes},
	Year = {2003}}

@article{pellegrini2004protein,
	Author = {Pellegrini, Matteo and Haynor, David and Johnson, Jason M},
	Date-Added = {2018-10-24 08:32:01 +0100},
	Date-Modified = {2018-10-24 08:32:01 +0100},
	Journal = {Expert review of proteomics},
	Number = {2},
	Pages = {239--249},
	Publisher = {Taylor \& Francis},
	Title = {Protein interaction networks},
	Volume = {1},
	Year = {2004}}

@phdthesis{lemmouchi2012etude,
	Author = {Lemmouchi, Slimane},
	Date-Added = {2018-10-23 23:12:05 +0100},
	Date-Modified = {2018-10-23 23:12:05 +0100},
	School = {Universit{\'e} Claude Bernard-Lyon I},
	Title = {Etude de la robustesse des graphes sociaux {\'e}mergents},
	Year = {2012}}

@inproceedings{guillaume2002web,
	Author = {Guillaume, Jean-Loup and Latapy, Matthieu},
	Booktitle = {Actes d'ALGOTEL'02 (Quatri{\`e}mes Rencontres Francophones sur les aspects Algorithmiques des T{\'e}l{\'e}communications)},
	Date-Added = {2018-10-23 21:11:29 +0100},
	Date-Modified = {2018-10-23 21:11:29 +0100},
	Title = {The Web graph: an overview},
	Year = {2002}}

@incollection{levene2004navigating,
	Author = {Levene, Mark and Wheeldon, Richard},
	Booktitle = {Web Dynamics},
	Date-Added = {2018-10-23 21:09:31 +0100},
	Date-Modified = {2018-10-23 21:09:31 +0100},
	Pages = {117--151},
	Publisher = {Springer},
	Title = {Navigating the world wide web},
	Year = {2004}}

@book{bac,
	Author = {Michel Rigo},
	Date-Added = {2018-10-22 13:20:43 +0100},
	Date-Modified = {2018-10-22 13:22:53 +0100},
	Publisher = {Universit{\'e} de li{\`e}ge, Facult{\'e} des sciences D{\'e}partement de math{\'e}matiques},
	Title = {Th{\'e}orie des graphesorie des graphes},
	Year = {2010}}

@manual{DUT,
	Author = {Ph. Roux},
	Date-Added = {2018-10-22 12:25:53 +0100},
	Date-Modified = {2018-10-22 12:28:24 +0100},
	Month = {f{\'e}vrier},
	Title = {Th{\'e}orie des graphes},
	Year = {2014}}

@techreport{Pres,
	Author = {Jean-Charles R{\'e}gin, Arnaud Malapert},
	Date-Added = {2018-10-22 10:57:49 +0100},
	Date-Modified = {2018-10-22 10:58:29 +0100},
	Title = {Th{\'e}orie des Graphes},
	Year = {2016}}

@book{IUTLyonInformatique,
	Date-Added = {2018-10-22 09:20:34 +0100},
	Date-Modified = {2018-10-22 10:55:04 +0100},
	Publisher = {IUT Lyon Informatique},
	Title = {Quelques rappels sur la th{\'e}orie des graphes},
	Year = {2012}}

@book{muller,
	Author = {Didier M{\"u}ller},
	Date-Added = {2018-10-22 09:13:39 +0100},
	Date-Modified = {2018-10-22 09:16:03 +0100},
	Publisher = {Commission romande de math{\'e}matique (CRM).},
	Title = {Introduction {\`a} la th{\'e}orie des graphes},
	Year = {2012}}

@article{karypis2000multilevel,
	Abstract = {


	Ce travaille propose un nouveau algorithme de partitionnement qui depasse l'algorithme de K=PM/LR (etat de l'art )dans le sperformances.

	Le probleme ici est de partitionner le graphe en k partie tq une fonction d'objective est optimiser. Une fonction objectif couramment utilis{\'e}e consiste {\`a} minimiser le nombre d'hyper-aretes  qui couvrent diff{\'e}rentes partitions. cependant, un certain nombre d'autres fonctions objectives sont {\'e}galement consid{\'e}r{\'e}es utiles.

	L'approche la plus couramment utilis{\'e}e pour calculer un partitionnement k-way est bas{\'e}e sur le paradigme de la bissection r{\'e}cursive (dichotomie), qui r{\'e}duit le probl{\`e}me du calcul d'un partitionnement k-way {\`a} celui de la r{\'e}alisation d'une s{\'e}quence de bissections. Le probl{\`e}me du calcul de la bissection optimale d'un hypergraphe est NP-hard  Cependant, de nombreux algorithmes heuristiques ont {\'e}t{\'e} d{\'e}velopp{\'e}s.

	Une nouvelle classe d'algorithme de bissection de graphe est apparue ,basee sur le paradigme multi-niveaux. Dans cette classe, une s{\'e}quence d'hyper graphes successivement plus petits (plus grossiers) est construite. Une bissection du plus petit hypergraphe est calcul{\'e}e. Cette bissection est ensuite projet{\'e}e successivement au niveau hypergraphe plus fin, et {\`a} chaque niveau, un algorithme de raffinement it{\'e}ratif est utilis{\'e} pour am{\'e}liorer encore la bissection.

	Plusieurs raisons ont fait que l'utilisation de la bissection recursive a ete abondonner. Premi{\`e}rement, un algorithme de bissection r{\'e}cursive ne nous permet pas d'optimiser directement des objectifs de nature globale et d{\'e}pend de la vue directe de toutes les k partitions. Deuxi{\`e}mement, un algorithme de partitionnement k-way est capable d'appliquer des contraintes d'{\'e}quilibrage plus strictes tout en conservant la capacit{\'e} d'explorer suffisamment l'espace de solution r{\'e}alisable pour optimiser l'objectif de partitionnement surtout lorsque la solution de partitionnement doit simultan{\'e}ment satisfaire {\`a} plusieurs contraintes d'{\'e}quilibrage. Troisi{\`e}mement, une m{\'e}thode qui obtient directement un partitionnement k-way peut potentiellement produire de meilleurs partitionnements qu'une m{\'e}thode qui calcule un partitionnement k-way via une bissection r{\'e}cursive.

	Le chercheurs depuis se sont mis a proposee de salgorithmes de partitionnement qui calcule le k-way artiotionnement directement les plus imporatants sont une genenralisation de l'algorithme FM pour partitionnement k-way. Le probleme est que cet algorithme sont les minimums locales. Les algorithmes K-PM / LFI r{\'e}cemment d{\'e}velopp{\'e}s par Cong et Lim [19] tentent de r{\'e}soudre ce probl{\`e}me en affinant un partitionnement en k-way en appliquant une s{\'e}quence de raffinement FM {\`a} 2 voies {\`a} des paires de domaines. L'appariement des domaines est bas{\'e} sur le gain de la derni{\`e}re passe, et le mouvement des cellules deux {\`a} deux se poursuit jusqu'{\`a} ce qu'il ne soit plus possible d'obtenir un gain suppl{\'e}mentaire. 

	N{\'e}anmoins, tous les partitionneurs ci-dessus ont tendance {\`a} produire des solutions inf{\'e}rieures {\`a} celles produites par les algorithmes de bissection r{\'e}currents {\`a} la pointe de la technologie, en particulier lorsqu'elles sont utilis{\'e}es pour optimiser un objectif pouvant {\^e}tre directement optimis{\'e} par le cadre de bissection r{\'e}cursif.

	L'algorithme proposee par els auteurs utilise le paradigme multi-niveau pour calculer un k-way partiotionnmenet et est facilement parrallelisable. 

	Definition formel du probleme :
	------------------------------------------ 
The k-way hypergraph partitioning pro- blem is defined as follows: Given a hypergraph G- (V,E) (where V is the set or vertices and E is the set of hyperedges) and an overall load imbalance tolerance c such that c _> 1.0, the goal is to partition the set V into k disjoint subsets,
V,V2,...,V such that the number of vertices in each set V is bounded by IVI/(ck) < Ivi[ clvI/k (contrainte de partitionnement ), and a function defined over the hyperedges is optimized (objective du partitionnemnt).

	Algorithme proposee : dite Â« multilevel k-way partitioning algorithm  Â»
	------------------------------
		Se compose principallement de 3 phases essenciels :

			1. La phase de Grossisement ou de  Coarsening : consite en le groupement de noeuds selon un critere pour former un hypergraphe. Deux schema peuvent etre appliquee dans cette pahse :

				a. edge-coarsenning : choisir les groupes les pairs de sommet qui appairaissent  dans plusieurs hyper-aretes. Les sommets sont visitee dans un ordre aleatoire. Pour chaque sommet v, tous les sommets non appari{\'e}s appartenant aux hypergers incidents {\`a} v sont pris en compte, et celui qui est connect{\'e} via l'arete ayant le poids le plus {\'e}lev{\'e} est associ{\'e} {\`a} v.

				b. hyperedge-coarsening : choisir l'ensemble le plus grand d'hyperaretes independantes (chaque sommet appartient a un seul groupe) et chaque groupe de noeuds d'une hyperarete sont groupees ensemble.
				
				Il est a notee que la 2eme approche est plus performantes selon les experiences.Le probl{\`e}me potentiel des  deux approche est que l'exigence d'ind{\'e}pendance (et dans une certaine mesure, de maximalit{\'e}) peut d{\'e}truire certaines grappes de sommets qui existent naturellement dans l'hypergraphe
				De ce fait, un nouveau schema a ete proposee  baptis{\'e} FirstChoice. Il est basee sur le edge-coarsening on prenant en compte toute les sommets et non pas les unmateched sommets uniquement.

			2. Phase de partitionnement : 
				Cette phase consiste en le parttiotionnement du graphe en respecetant les contrainte du probleme et en optimisant la fonction objective. Une facon d'obtenir une k-way prtition est de de repeter la phase 1 jusqu a obtention de k sommets (contient des inconvenients ). 
				Dans l'algorithme proposee ici, le pakcage hmetise est utlisee qui un multi niveau algo de bissection pour les hypergraphes.

			3.Phase de Uncoarsening : 
				Durant cette phase, un partitionnement de l'hypergraphe plus grossier est projet{\'e} successivement vers le niveau suivant et un algo de rafinnement est appliquee pour optimiser la fonction objective sans bien evidemment violer les contraintes du probleme. L



Definitions :

	- Given a k-way partitioning and a hyperedge e, the external degree of e is defined to be 0, if e is not cut by the partitioning, otherwise it is equal to the number of partitions that is spanned by e.

	- 
















































},
	Author = {Karypis, George and Kumar, Vipin},
	Date-Added = {2018-10-17 20:22:24 +0100},
	Date-Modified = {2018-10-28 22:02:26 +0100},
	Journal = {VLSI design},
	Number = {3},
	Pages = {285--300},
	Publisher = {Hindawi},
	Title = {Multilevel k-way hypergraph partitioning},
	Url = {https://www.cs.york.ac.uk/rts/docs/DAC-1964-2006/PAPERS/1999/DAC99_343.PDF},
	Volume = {11},
	Year = {2000},
	Bdsk-Url-1 = {https://www.cs.york.ac.uk/rts/docs/DAC-1964-2006/PAPERS/1999/DAC99_343.PDF}}

@article{hespanha2004efficient,
	Abstract = {



									Spectral Clustering

	Le cas. des graphes non orientee 

	Dans ce travaille, une analyse du spectre de la matrice d'adjacence est effectuee en se basant sur ses valeurs/vecteurs propores.

	Un vecteur de partition est un vecteur de taile |V| dont chaque entree represente le num de partition auquel le noeuds est affectee.
	Une matrice de k-partition H est une matrcie dont les valeurs sont binaire et chaque colonne ci represente par 1 les noeuds de la partition Vi et 0 les autres noeuds. De ce fait,on aura que H'.H =diag{|Vi|}.
	Le cout d;une partition P est C(P) = 1â€²nA1n âˆ’ trace(Î â€²AÎ ), ou A est la matrice des couts du graphe et 1n est une vecteur dont les valeurs sont des 1.

	Le probleme  d'optimisation peut alors se formuler ainsi :
		maximize trace(Î â€² AÎ ) subject to Ï€vj âˆˆ {0, 1}, âˆ€v, j
		Î  orthogonal 
		1nÎ 1k = n 
		1nÎ ei â‰¤ l, ei est le ieme vecetur de la base canonique

	Dans ce travaille on reduit la resolution aux cas ou la somme des couts des ses aretes est = 1 (Normalisation : lorsque le degree est elevee alors le couts doit etre petit dans la moyenne des cas) dasn ce cas la matrice est dite doublement stochastique.

	On suppose que Î  := UZ.Il propose pou resoudre ce probleme 2 algo:
		- Clustering algorithme : pour calculer le Z avec une heuristique permettant de respecter le meiux les conditions 2 et 3 
		- Algorithme de projection : Pour calculer le Î  optimale 

Rappel : Sur le spectre d'une matrice

	- valeur propre Av=lamda.v ==> lamda valeur propre
	- A = Q-1 . B . Q ou B est une matrice diagonale et les bii = valeurs propre de A et Q est une matrice dont les colonnes sont des vecteurs propres


Note :

	- Le code est fourni en annexe _MatLab Code_},
	Author = {Hespanha, Joao P},
	Date-Added = {2018-10-17 18:53:23 +0100},
	Date-Modified = {2018-10-20 22:48:21 +0100},
	Journal = {Santa Barbara, CA, USA: University of California},
	Publisher = {Citeseer},
	Title = {An efficient matlab algorithm for graph partitioning},
	Year = {2004}}

@article{blondel2008fast,
	Abstract = {


	Le principe de cette methode est le suivant :
		-l'algo est composee de deux phase 
			- Phase01 : le temp d'execution de cette phase depend enormement de l'ordre des noeuds

				- chaque noeuds est considere comme une communautee 
				- Pour chauque noeud i on considere ses voisins j 
					 on evalue le gain de modularite apportee en supprimant le noeud i et on le rajoutant a la communautee de j
				- On rajoute i a la communautee apportant max gain positive si aucun gain n est positive alors x reste dans sa communautee 

			- Phase 02 : consiste en la construction d'un nouveau graphe dont 
					- les neouds sont les communautee de la phase 01
					- le poids des aretes = 
							- la somme des poids des aretes liant les noeuds des deux communautee 
							- les aretes interne aux communautee deviennet des boucles dont le poids est la somme tjrs
			
			- reappliquer la phase 01 jusqu a ce soit :
					- plus de changement
					- maximum de modularite est atteint
					},
	Author = {Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
	Date-Added = {2018-10-17 18:05:37 +0100},
	Date-Modified = {2018-10-17 18:20:23 +0100},
	Journal = {Journal of statistical mechanics: theory and experiment},
	Number = {10},
	Pages = {P10008},
	Publisher = {IOP Publishing},
	Title = {Fast unfolding of communities in large networks},
	Volume = {2008},
	Year = {2008}}

@inproceedings{giatsidis2011evaluating,
	Abstract = {


	Les sous-graphe de communautee sont caracterisee par leur liens dense. La mesure de la qualitee d'une communautee est tres importante. En effet plusieurs mesure d'evaluation existe dans lalitterature, ce travaille evalue les communautee en se basant sur le concepte de k-core et propose une extention de cette mesure aux cas des graphes ponderes.

	1. Algorithme pour extraction du k-core :
		IN: graphe non oriente D , un nombre positive k
		OUT : k-core(D)
		Complexite :  O(kn) steps ==> peut etre appliquer aux graphe dense 
			1. F= D
			2.Supprimer tt les neouds ayant un degre < k

	




Definitions:
---------------
	- We denote by âˆ†(G) the minimum degree of a vertex in G
	- we define the k-core of a graph as the maximum size subgraph H of G where âˆ†(H) â‰¥ k , where k is a positive integer.
		en terme plus simple c'est le graphe initiale en iliminant les noeuds ayant un degree < k .

	- The degeneracy of G is defined as follows.  Î´âˆ—(G) = max{âˆ†(H) | H âŠ† G}.

	- (Vertex core number) The core number of a vertex v of G is the maximum k for which v belongs in the k-core of G.
	- The core number of a subset S of the vertices of G is the maximum k for which all vertices of S belong in the k-core of G


Remarque:
---------------
	- Dans le but de quantifier la  notion d'importance d'un n{\oe}ud dans un graphe, les chercheurs ont propos{\'e} plusieurs d{\'e}finitions connues sous le nom de mesures de centralit{\'e}


Beatiful Expressions : _To use in the abstract_

	-  In all cases, due to the economic aspects of these networks, the ranking of individual nodes is a cornerstone concept.},
	Author = {Giatsidis, Christos and Thilikos, Dimitrios M and Vazirgiannis, Michalis},
	Booktitle = {Advances in Social Networks Analysis and Mining (ASONAM), 2011 International Conference on},
	Date-Added = {2018-10-17 15:49:25 +0100},
	Date-Modified = {2018-10-27 23:39:31 +0100},
	Organization = {IEEE},
	Pages = {87--93},
	Title = {Evaluating cooperation in communities with the k-core structure},
	Url = {https://www.researchgate.net/profile/Michalis_Vazirgiannis/publication/221273470_Evaluating_Cooperation_in_Communities_with_the_k-Core_Structure/links/56b89ce608ae44bb330d355c/Evaluating-Cooperation-in-Communities-with-the-k-Core-Structure.pdf},
	Year = {2011},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Michalis_Vazirgiannis/publication/221273470_Evaluating_Cooperation_in_Communities_with_the_k-Core_Structure/links/56b89ce608ae44bb330d355c/Evaluating-Cooperation-in-Communities-with-the-k-Core-Structure.pdf}}

@inproceedings{kang2011beyond,
	Abstract = {

	Le travaille present repond a deux questions importantes : comment compressee un graphe efficacement ?comment detecter les communautee dans un grpahe donnee? qui sont deux questions en etroite relation. En effet, si on trouve les bonnes communautee alors on aura une meilleur compression vues que les neouds d'une d'une mm communautee contiennent des informations redondantes.

	Au debut l'approche utilisee est celle du Â« cave Man Â» ou on considere qu'un individue connait bocoup d'informations sur  les individue (noeuds) qui sont avec lui dans une meme caverne (clique) et peu d\informations sur les individue dans les autres cavernes. Cependant, les graphes du monde reel n'obeissent pas generalement a cette regle. Ils suivent une distributions de degr{\'e}s de loi de puissance avec peu de n{\oe}uds "hub" ayant des degr{\'e}s tr{\`e}s {\'e}lev{\'e}s et la majorit{\'e} des n{\oe}uds ayant des degr{\'e}s faibles. Cette constation a remis en cause le principe du  Â« cave man Â» car les hub sont connectee a la majoritee des noeuds sonnant ainsi une large caverne.

	La methode proposee, baptisee SLASHBURN, est basee sur l'exploitation les hub et leurs visinage en paratant du fait que les graphe reel peuvent etre facilement partitionner par ces hubs. Les communautees defini ainsi sont caracterisee dans la matrice d'adjacence par des clocks maigre (!!!!). Ce travaille montre que l'ordonnoncementde ces noeuds d'une maniere prudente permet d'avoir une representation compacte de matrcie d'adjacence.

	Le princpe de cette methode consiste en 3 etapes :
		1. Algorithm 1: SLASHBURN
I			Input: Edge set E of a graph G = (V,E), a constant k(default = 1).
			Output: Array Î“ containing the ordering V â†’ [n].
				1: Remove k-hubset from G to make the new graph Gâ€².
					Add the removed k-hubset to the front of Î“.
				2: Find connected components in Gâ€². Add nodes in
					non-giant connected components to the back of Î“, in the decreasing order of sizes of connected components they belong to.
				3: Set G to be the giant connected component(GCC) of Gâ€². Go to step 1 and continue, until the number of nodes in the GCC is smaller than k.

	Complexite :
		- temporelle  : O(|E| + |V |log|V |)i ou i est le nombre d'iterations 
		- spaciale :	O(V ) 



Information Supplementaire :

	- HADOOP, an open source MAPREDUCE framework can be used to  show the performance implication of an algo for large scale graph mining on distributed platform,

	- LiveJournal is one of the dataset that is very hard to compress

	- real world, power-law graphs and ran- dom(Erdo Ì‹s-Re Ìnyi [15]) graphs? 

},
	Author = {Kang, U and Faloutsos, Christos},
	Booktitle = {Data Mining (ICDM), 2011 IEEE 11th International Conference on},
	Date-Added = {2018-10-17 12:18:32 +0100},
	Date-Modified = {2018-10-17 15:36:18 +0100},
	Organization = {IEEE},
	Pages = {300--309},
	Title = {Beyond'caveman communities': Hubs and spokes for graph compression and mining},
	Year = {2011}}

@article{hernandez2014compressed,
	Author = {Hern{\'a}ndez, Cecilia and Navarro, Gonzalo},
	Date-Added = {2018-10-17 10:15:10 +0100},
	Date-Modified = {2018-10-17 10:15:23 +0100},
	Journal = {Knowledge and information systems},
	Number = {2},
	Pages = {279--313},
	Publisher = {Springer},
	Title = {Compressed representations for web and social graphs},
	Url = {http://repositorio.uchile.cl/bitstream/handle/2250/126521/Compressed%20representations%20for%20web%20and%20social%20graphs.pdf?sequence=1},
	Volume = {40},
	Year = {2014},
	Bdsk-Url-1 = {http://repositorio.uchile.cl/bitstream/handle/2250/126521/Compressed%20representations%20for%20web%20and%20social%20graphs.pdf?sequence=1}}

@inproceedings{buehrer2008scalable,
	Author = {Buehrer, Gregory and Chellapilla, Kumar},
	Booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
	Date-Added = {2018-10-17 10:12:39 +0100},
	Date-Modified = {2018-10-17 10:13:38 +0100},
	Organization = {ACM},
	Pages = {95--106},
	Title = {A scalable pattern mining approach to web graph compression with communities},
	Url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.9435&rep=rep1&type=pdf},
	Year = {2008},
	Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.9435&rep=rep1&type=pdf}}

@article{liu2015empirical,
	Abstract = {

												                     VoG Overlapp
Methode proposee est basee sur le k-core . L'algorithme de base est :

2. Algorithm 2 KCBC: k-core-based Graph Clustering
		Input: graph G
		While the graph is nonempty
			Step 1: Compute core numbers (max k for which the node is present in the decomposition) for all nodes in the graph.
			Step 2: Choose the maximum k (kmax) for which the decomposition is non-empty, and identify nodes which are present in the decomposition as the ``decomposition set.'' Terminate when kmax = . 
			Step 3: For the induced subgraph from the decomposition set, identify each connected component as a structure.
			Step 4: Remove all edges in the graph between nodes in the decomposition set---they have been identified as structures already. 
		return set of all identified structures


Comparaison entre 5differentes technique de detection de communitee et de clustering en terme de la taille et de la qualite du resultat de ses dernieres. Les cinq methode sont : METIS, Louvain, spectral clustering, SlashBurn et la methode proposee dans cet article k-core-based clustering notee KCBC.

},
	Author = {Liu, Yike and Shah, Neil and Koutra, Danai},
	Date-Added = {2018-10-17 10:10:12 +0100},
	Date-Modified = {2018-10-27 23:41:59 +0100},
	Journal = {arXiv preprint arXiv:1511.06820},
	Title = {An empirical comparison of the summarization power of graph clustering methods},
	Url = {https://arxiv.org/pdf/1511.06820.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1511.06820.pdf}}

@article{liureducing,
	Author = {Liu, Yike and Safavi, Tara and Shah, Neil and Koutra, Danai},
	Date-Added = {2018-10-17 10:09:26 +0100},
	Date-Modified = {2018-10-17 10:09:36 +0100},
	Title = {Reducing Million-Node Graphs to a Few Structural Patterns: A Unified Approach},
	Url = {http://www.mlgworkshop.org/2016/paper/MLG2016_paper_29.pdf},
	Bdsk-Url-1 = {http://www.mlgworkshop.org/2016/paper/MLG2016_paper_29.pdf}}

@inproceedings{shah2015timecrunch,
	Author = {Shah, Neil and Koutra, Danai and Zou, Tianmin and Gallagher, Brian and Faloutsos, Christos},
	Booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2018-10-17 10:08:37 +0100},
	Date-Modified = {2018-10-17 10:08:52 +0100},
	Organization = {ACM},
	Pages = {1055--1064},
	Title = {Timecrunch: Interpretable dynamic graph summarization},
	Url = {https://web.eecs.umich.edu/~dkoutra/papers/Timecrunch_KDD15.pdf},
	Year = {2015},
	Bdsk-Url-1 = {https://web.eecs.umich.edu/~dkoutra/papers/Timecrunch_KDD15.pdf}}

@inproceedings{dunne2013motif,
	Author = {Dunne, Cody and Shneiderman, Ben},
	Booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	Date-Added = {2018-10-17 10:07:52 +0100},
	Date-Modified = {2018-10-17 10:07:52 +0100},
	Organization = {ACM},
	Pages = {3247--3256},
	Title = {Motif simplification: improving network visualization readability with fan, connector, and clique glyphs},
	Year = {2013}}

@article{brisaboa2014compact,
	Abstract = {	
	Ce travaille est une continuitee de Â« k2-trees for compacte web graph representattion Â».


Definitions:

	- Graphe du Web : est un graphe orientee ou chaque noeud represente une page web et les aretes represente les hyperliens entre les pages. 

Information Supplementaire :
	
	- HIts : l'un des plus important algorithme pour trouver les hubs et les autoritee sur le web. Commence par selectionner un ensemble aleatoire de page et trouver les sous graphes induit par ces pages qui ne sont rien d'autre que les pages accessibles via ces noeuds ou les predecceurs de ces noeuds.},
	Author = {Brisaboa, Nieves R and Ladra, Susana and Navarro, Gonzalo},
	Date-Added = {2018-10-03 18:52:20 +0100},
	Date-Modified = {2018-10-03 19:23:51 +0100},
	Journal = {Information Systems},
	Pages = {152--174},
	Publisher = {Elsevier},
	Read = {1},
	Title = {Compact representation of web graphs with extended functionality},
	Url = {http://repositorio.uchile.cl/bitstream/handle/2250/126520/Compact%20representation%20of%20Webgraphs%20with%20extended%20functionality.pdf?sequence=1},
	Volume = {39},
	Year = {2014},
	Bdsk-Url-1 = {http://repositorio.uchile.cl/bitstream/handle/2250/126520/Compact%20representation%20of%20Webgraphs%20with%20extended%20functionality.pdf?sequence=1}}

@article{seo2018effective,
	Author = {Seo, Hojin and Park, Kisung and Han, Yongkoo and Kim, Hyunwook and Umair, Muhammad and Khan, Kifayat Ullah and Lee, Young-Koo},
	Date-Added = {2018-10-03 11:00:47 +0100},
	Date-Modified = {2018-10-03 11:00:47 +0100},
	Journal = {The Journal of Supercomputing},
	Pages = {1--15},
	Publisher = {Springer},
	Title = {An effective graph summarization and compression technique for a large-scaled graph},
	Year = {2018}}

@article{khan2017faster,
	Abstract = {

	Compression de graphe ponderee est utile pour modeliser des situation de la vie reel. En effet, une compression qui ne prend pas en compte les poids des arets ne peut pa modeliser les relation fortes et faibles entre les noeuds .
	
	Cet article propose la solution a deux probleme majeurs qui sont : BWGS et PWGS. 
	Pour minimiser les calcul, on utilise  LSH qui aide a restreindre le calcul de similarite aux noeuds les plus propable d'etre similaire. La mesure de similarite est calculee avec le coefficient de Jaccard.
	
	L'approche BWGS ne peut etre utilisee en tant que tel pour PWGS car elle ignore les poids, qui ont une importance dans la mesure d'influence, des aretes durant la phase de hachage. C'est pourquoi les auteurs proposent un autre schema de hashage pour le PWGS. De ce fait, ils ont obtenue une solution qui n'en seulement permet de realiser la compression dans un temps optimale mais aussi de prendre en compte les poids des differents noeuds.

	Pour trouver l'ensemble formant un super- noueds dans le graphe et dans le cas des deux approches, on choisit un noeuds q et on choisiit les noeuds qui ont un EDR > seuil (si proche de 0.5 => mauvise compression et si trop petit => des erreurs). 


	Le BWGS : dont le but est de calculer Sg dans un temps raisonnable et de maniere consice pour qu'il suppote l'execution des differents algorithmes d'analyse en memoire centrale. Pour repondre au premier besoin on rassemble a chauq iteration un ensemble de noeuds et non pas une pair uniquement tant dis que que pour repondre au deuxieme besoin on utilise le taux de compression cr(Sg) = |Es| / |E| . Le probleme donc se formule ainsi :

			Prob 01 : Etant G un graph non orientee et un seuil de compression 0<cr<1 , trouver Sg dans un temp lineaire ayant un cr(Sg)<cr  ,une ditance minimale avec G et qui introduit un min d'erreurs d'aretes. 

		- Creating a summary graph for the BWGS problem
			- Validating the use of the Jaccard coefficient 
					
			- Locating compressible nodes by using LSH
			- Merging compressible nodes
				apres avoir regroupee les noeuds en se basant sur LSH, on choisie aleatoirement a chauqe iteration un noeuds q et les noeuds qui apparaisssent avec lui dans la table de hashage = ScandNq qu'on ordonne selon EDR et iterativement on elimine les noeuds qui viole les contraintes de seuil et la difference de piods =SCompNq. On observe que si on fusionnne les noeuds ScompNq directement on obtient un taux d'erreurs elevees c'est pourquoi on fusionne les neouds les plus similaire dans un premier lieu puis on rajoute les autres sequentiellement (certain noeuds sont eliminer psk le calcul EDR est influencer par la fusion des noeuds precedent)



	PWGS: BWGS ne permet d'analyser l'influence dans G car il est construit en se basant sur la similiratee de voisinage et non l'influence qui est represetentee par les poids des artes. En effet , chaque poids wi de u vers v montre le degree d'influence de v sur u. 

			Prob 02 : Etant Pg un graph personnalisee et un noeuds influenceur vinf , trouver son compressee Sg ayant k super noeuds et l super-arets  ,une ditance minimale avec G et qui introduit un min d'erreurs d'aretes. 

		- Creating a summary graph for the PWGS problem
			La taille de Sg est depend fortement de la taille de Pg c'est pourkoi on ne prends en compte que k supernoueds et l super-arets qui sont, pour les noeuds trouver avec DFS BFS q partie de vinf ,et pour les poids sont calculer en tant que debit avec equation 4 de l'article.

			














Questions : 
	- dans la definition du probl 02 ,j'ai pa compri le k noeuds et l aretes ca concerne qui esk Pg ou Sg ??
	- contradiction de la definition 02 avec explication qui viens avant !!!!









Definitions:

	- Le compressee : The summary SG(Vs,Es,ws) of a weighted graph G(V,E,w) is a compressed representation whose nodes are called super nodes Vs and edges are referred to as super edges Es. w of every (u,v)âŠ†E represents the edge weight, whereas the semantics of ws (us , vs ) âŠ† Es differ for our two types of summarization problems. 

	- Chaque super noeud vs âˆˆ Vs contient soit une paire ou un ensemble de noeuds {v1,...,vl}âˆˆV ou il est preservee comme il est , i.e., vs âˆˆVs â‡’ vs âˆˆV. De facon similaire, les aretes sont agregger pour former des super arete dans SG.
	- Chaque super noeuds represente une clique,bi-clique, ou un. sous graphe bipartie
	- La creation des super noeuds peut introduire des erreurs de deux types : 
			|------> insertion de nouvelle aretes : 
			|------> suppresssion d'aretes existante si il viole les conditions: EDR > seuil

	- (Personalized graph). Given a directed graph G and an influential node vinf âˆˆV, a personalized graph PG(Vp,Ep,wp) is a directed subgraph of G where every vp âˆˆVp â‡’ vp âˆˆV and is accessible from vinf.
	- Un super neuds est ensemble de neouds ayant un voisinage simialire et une difference de poids minimale pour chaque arete.

	- (Set of Candidate Similar Nodes). Given a set of nodes S = {v1, .., vi, .., vm} âˆˆ V, q âˆˆ V, an EDR threshold tN, and an edge weight difference threshold tEw, we call S the Set of Candidate Similar Nodes SCandNq if the EDR of any vi âˆˆ S with q is below tN or their difference in edge weights is above tEw.

	- (Set of Compressible Nodes). Given an SCandNq, qâˆˆV, an EDR threshold tN, and edge weight difference thresh- old tEw, its largest subset is the Set of Compressible Nodes SCompNq if the EDR of every vi âˆˆ SCandNq with q is above tN and their difference in edge weights is below tEw.

},
	Author = {Khan, Kifayat Ullah and Dolgorsuren, Batjargal and Anh, Tu Nguyen and Nawaz, Waqas and Lee, Young-Koo},
	Date-Added = {2018-10-03 10:57:36 +0100},
	Date-Modified = {2018-10-05 12:41:04 +0100},
	Journal = {Information Sciences},
	Pages = {237--253},
	Publisher = {Elsevier},
	Title = {Faster compression methods for a weighted graph using locality sensitive hashing},
	Url = {https://www.researchgate.net/profile/Kifayat_Khan4/publication/318768173_Faster_Compression_Methods_for_a_Weighted_Graph_using_Locality_Sensitive_Hashing/links/59c067c80f7e9b48a29bb086/Faster-Compression-Methods-for-a-Weighted-Graph-using-Locality-Sensitive-Hashing.pdf},
	Volume = {421},
	Year = {2017},
	Bdsk-Url-1 = {https://www.researchgate.net/profile/Kifayat_Khan4/publication/318768173_Faster_Compression_Methods_for_a_Weighted_Graph_using_Locality_Sensitive_Hashing/links/59c067c80f7e9b48a29bb086/Faster-Compression-Methods-for-a-Weighted-Graph-using-Locality-Sensitive-Hashing.pdf}}

@inproceedings{brisaboa2009k,
	Abstract = {

	Plusieurs algorithmes existent dans la litterature pour explorer les structures de graphe mais la problematique posee de nos jours est celle de la grande masse de donnee (BIG DATA). Ce qui a poussee plusieurs chercheurs a considere la compression de graphe qui a pour but d'offrir une representation compacte permettant une navigation rapide sans decompression. De ce fait, les algorithmes classiques pourront etre executee dans la memoire centrale meme si la taille du graphe est tres importante.

	Ce travail exploitent les propriete de la matrice d'adjacence avec une technique generaliste tirant profis du clustering. La representation proposee permet de faire un parcours directe ou inverse du graphe. Ces derniere sont en nombre de 3:
	- localitee de reference : plusieurs 1 sont placee autour de la diagonale
	- copy property : les lignes similaires sont commun dans la matrice
	- asym{\'e}trie de distribution : quelques colonnes ont pluseiurs 1 mais la plupart ont quelques 1 (matrice creuse).
	
	Dans cette approche la matrice d'adjacence est reporesentee par un k2-tree qui est un arbre dont chaque noeuds contient un seul bit de donnee: 1 pour les noeuds interne et 0 pour les feuilles a l'exception des de dernier niveau ou les valeurs des noeuds representent des valeurs de la matrice. Chaque noeuds possedent  possede k2 fils si sa valeur est 1 et 0 fils sinon.Deux cas se presente, si n=k2 alors alors on divise la matrice en des sous matrice de taille k2 sinon on complete la matrice avec des 0 (a droite et en bas ) pour arriver a la prochaine puissance carree. Le resultat est dons k2 sous matrices dont chacune represente un noueds fils dans l'arbre de valeurs 0 si toutes ses cellules sont nulles et de valeurs 1 sinon.

	L'arbre est representee en memoire sous forme de deux tableax :
		- T : contenant les bits des noeuds ordonnee par niveaux  a l'exception du dernier niveau
		- L : contenant les bits du derniers niveaux.
	Les algorithmes de parcours directe et inverse sont enoncee dans l'article et sont facile comprendre. Le choix du k affecte les performances de cet aproche. En effet, plus le k est petit plus on a besoin d'espace memoire et moins de matrices on aura. De ce fait, les auteurs propose une hybridation par rapport au valeurs de k ou des grandes valeurs de k seront utilisee pour les premiers niveaux et de petite valeur de k pour les derniers niveaux de l'arbre.
	
Definitions:

	- Matrice d'adjacence du Graphe du Web: est une matrice carree de {aij} de taille nxn, ou chaque ligne ou colonne represente une page web. Une cellule aij=1 si il y a un hyperlien dans la page i vers la page j et 0 sinon.

	- la complexitee au pire des cas est : logk2(n2) noeuds => k2 m logk(n).


NB: 
	- On part de l'hypothese que les pages web sont ordonnee par ordre alphabetique ce qui met les pages du meme domaine dans des positions proches dans la matrice

Citation Importante:
 
	- Web Graphe Framework de Boldi and vigna.
	- Claude and navaro Re_pair for webGraph. with reverse navigation },
	Author = {Brisaboa, Nieves R and Ladra, Susana and Navarro, Gonzalo},
	Booktitle = {International Symposium on String Processing and Information Retrieval},
	Date-Added = {2018-10-03 10:56:33 +0100},
	Date-Modified = {2018-10-04 00:26:02 +0100},
	Organization = {Springer},
	Pages = {18--30},
	Title = {k 2-trees for compact web graph representation},
	Url = {https://pdfs.semanticscholar.org/7d98/d729058067dcd520ce670c83842cafa289a0.pdf},
	Year = {2009},
	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/7d98/d729058067dcd520ce670c83842cafa289a0.pdf}}

@article{koutra2015summarizing,
	Abstract = {
	

	L'idee de ce travaillle est de construire un vocabulaire des types de sous-graphes (etoile, clique, chaines,bi-clique) qui apparaissent frequemment dans les graphe reel et de trouver la representation la plus succincte d'un graphe en terme de ce vocabulaire qui est : cliques, near-cliques, stars, chaines, near-bipartite, cores. L'evaluation de cette methode ce fait a travers le principe MDL ce qui permet d'avoir une efficacitee independante des parametre d'entree. 


	La problematique 01 est donc de trouver a partir d'un graphe en entree, la liste des sous-graphe, eventuellement qui se chevauchent (noeuds en commun mais non arete en commun), qui decrit le graphe de la maniere la plus succincte.

	La problematique 02 est de trouver le model M le plus petit en terme de longueur de bits pour representer G dont la longueur est de L(G,M) = L(M) + L(E) , a partir du graphe G, sa matrice d'adjacence A et un vocabulaire de structure.
	
	L'ensemble des modele M' est la permutation de tout les sous-ensembles possibles de de C = union  {tout les structure possible d'un type x}

	    1. Pour l'encodage du modele M on utilise la formule suivante:

	    		L(M) = LN( |M|+1 ) + log			+ 
	    				|			|			|--->pour chaque structure s on encode son type x(s) avec un code de prefixe optimale et sa structure
	    				|			|------------------------> encodage du nombre de structure de chaque type x dans le modele M
	    				|---------------------------------------------> Nombre de structure dans le modele M
	    	    L'encodage du modele depends de la longueur des structure s notee L(s) qui se calcule selon le type de la structure x(s):
	    		- s = Clique : L(s) = LN( |s| ) +log(n |s|) // encodage du nombre de noeuds + leurs ids
	    		- s =  near-clique : L(s) = LN( |s| ) +log(n |s|) + {\ldots}{\ldots} // encodage du nombre de noeuds + leurs ids + nombre d'arete presente et le nombre d'arete manquante. c'est un encodage exacte 
	    		- s =  Core Bipartie : A et B sont deux cores non vides et ne possedant aucun noeuds en commun. ayant des aretes externe de l'un vers l'autre mais aucune arete interne. // la taille de A et la taille de B ainsi que les ids des noeuds.
	    		- s =  near Core Bipartie :  // la taille de A et la taille de B ainsi que les ids des noeuds +  nombre d'arete presente et le nombre d'arete manquante. c'est un encodage exacte 
	    		- s =  Etoile : cas particulier des core bi-partie ou A= noeuds centrale et B= reste // Le nombre de noeuds externe + identification du hub + identification des spokes ????
	    		-s =  chaine : // la longueur de la chaine + identification des noeuds dans l'ordre

	    2.Pour l'erreure E : E=E+. et E- ou E+ = la partie de A modeliser par M et qui contient des aretes superflux et E- =la partie de A non modeliser par M et dont M manque d'aretes 
			L(E) = L(E-) + L(E+) 

	L'algorithme est constituee principalement de 3 etapes essetiels :
		- Generation des sous-graphes : technique de clustering ou de detection de communautee peuvent etre utilisee. Dans le cas de l'article, la methode utilisee est SLASHBURN.

		- Etiquetage des sous-graphes :
				- Etiquetage des structure parfaite : 
					- Le teste pour les cliques ou les chaines est basee sur les le degree de distribution
					- Bipartie si la sa valeur propre maximale est minimale sont de meme ordre de grandeur ( =? )
							|---> pour trouver les deux xensembles de noeuds on utilise BFS avec coloriage des noeuds . si l'un des ensemble ne contient qu'un seul element alors c'est une etoile et non un bipartie
				-  Etiquetage des structure approximative : 
					- le sous graphe est encodee dans tout les cas possible et la structure ayant un cout minimale en terme de MDL et conservee 

		- l'assemblage : Le probleme de trouver M parmis M' est NP-difficile c'est pourquoi on a recours a des heuristiques .
			pour reduire l'espace de recherche, on associe a chauqe structure condidate une mesure de qualitee == nbre de bits gagnee en codant le sous graphe comme une structure x
			Les heuristics constituee sont :
				- Plain : approche de base donne toutes les structures condidates comme summary
				- TOP -k : Selectionne les k meilleurs condidats qui sont ordonnee dans un ordre decroisssant de la mesure de qulitee
				- GREEDY'NForget : considere chaque structure condidate sequentiellement et l'inclue dasn le modele si elle n'augemente pas son cout. Il est a notee que cette heuristique est couteuse en temps de calculs et donne de meilleurs resulats avec les petite instance de sous graphe.
			
	VoG utilsent les trois heuristiques et choisie la meilleurs compression. Il a une complexite temporelle presque lineaire par rapport au nombre de noeuds.
	
	Le but de ce travaille n'est pas de faire un compression mais d'extraire les motifs les plus signifiant d'un graphe en utilisant la compression comme le moyen le plus efficace.

Acronyme:
	- VoG: Vocabulary-based-summarization of Graphs.
	- MDL : Minimum Description Length

NB:
	- Cette compression est applicable aux graphes non-orientee et sans boucles mais peut etre generalisee.
	- Dans la matrice d'adjacence :
		- Une clique : une sous matrice carree plaine de 1
		- Near-clique : une sous matrice carree dont la plupart des valeurs sont des 1
		- etoile : un L inversee
		- chaine : ligne parallele a la diagonale

	- Erreur E = M xor A d'ou on a un algorithmes de compression sans perte.

	-LN est le codage MDL le plus optimale pour representer les entiers >=1 
	
},
	Author = {Koutra, Danai and Kang, U and Vreeken, Jilles and Faloutsos, Christos},
	Date-Added = {2018-10-03 10:55:19 +0100},
	Date-Modified = {2018-10-31 11:28:47 +0100},
	Journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	Number = {3},
	Pages = {183--202},
	Publisher = {Wiley Online Library},
	Title = {Summarizing and understanding large graphs},
	Url = {https://arxiv.org/pdf/1406.3411.pdf},
	Volume = {8},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1406.3411.pdf}}
